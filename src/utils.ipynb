{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Mean & Std\n",
    "\n",
    "Calculates mean and std of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(dataset):\n",
    "    mean = dataset.data.mean(axis=(0, 1, 2)) / 255.\n",
    "    std = dataset.data.std(axis=(0, 1, 2)) / 255.\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "\n",
    "Splits dataset into multiple subsets.\n",
    "\n",
    "### TODO\n",
    "\n",
    "* bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split_by_dist(dataset, size: int, params: dict = {}):\n",
    "    \"\"\"Returns subsets of `dataset`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: datasets\n",
    "        By torchvision.datasets.\n",
    "    size: int\n",
    "        Number (Length) of subsets.\n",
    "    params: dict\n",
    "        Contains `distFunc` which returns np.array.\n",
    "        Sum of returned array SHOULD be 1.\n",
    "    \"\"\"\n",
    "\n",
    "    assert size != 0, \"`size` > 0\"\n",
    "\n",
    "    if 'distFunc' not in params:\n",
    "        params['distFunc'] = uniform\n",
    "\n",
    "    distFunc = params['distFunc']\n",
    "\n",
    "    # calculates distribution\n",
    "    dist = distFunc(size, params)  # dist: np.array\n",
    "    assert math.isclose(sum(dist), 1.), \"sum of `dist` shoule be 1.\"\n",
    "\n",
    "    N = len(dataset)\n",
    "    result = np.full(size, N) * dist\n",
    "    result = result.astype('int')  # to integers\n",
    "    # adjustment for that summation of `result` SHOULD be `N`\n",
    "    result[-1] = N - sum(result[:-1])\n",
    "\n",
    "    return random_split(dataset, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(size: int, params: dict = {}):\n",
    "    return np.ones(size) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(size: int, params: dict = {}):\n",
    "    \"\"\"Returns normal (Gaussian) distribution\n",
    "\n",
    "    Uses `abs` to restrict to non-zeros.\n",
    "\n",
    "    In fact, it is not a normal distribution because there are only\n",
    "    positive elements in `result`.\n",
    "\n",
    "    See https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        Number (Length) of chunks.\n",
    "        Same as length of returned np.array.\n",
    "    params: dict\n",
    "        Contains 'loc', 'scale', 'lower' and 'upper'.\n",
    "        The latter two are lower-bound and upper-bound respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'loc' not in params:\n",
    "        params['loc'] = 0.\n",
    "    if 'scale' not in params:\n",
    "        params['scale'] = 1.\n",
    "    if 'lower' not in params:\n",
    "        params['lower'] = 0.\n",
    "    if 'upper' not in params:\n",
    "        params['upper'] = None\n",
    "\n",
    "    loc, scale, lower, upper = params['loc'], params['scale'], params['lower'], params['upper']\n",
    "\n",
    "    result = np.random.normal(loc, scale, size)\n",
    "    result = abs(result)  # `result` SHOULD be only positive.\n",
    "    result = result.clip(lower, upper)\n",
    "    return result / sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto(size: int, params: dict = {}):\n",
    "    \"\"\"Returns Pareto distribution\n",
    "\n",
    "    See https://numpy.org/doc/stable/reference/random/generated/numpy.random.pareto.html .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        Number (Length) of chunks.\n",
    "        Same as length of returned np.array.\n",
    "    params: dict\n",
    "        contains 'alpha', 'lower' and 'upper'.\n",
    "        The latter two are lower-bound and upper-bound respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'alpha' not in params:\n",
    "        params['alpha'] = 1.16  # by 80-20 rule, log(5)/log(4)\n",
    "    if 'lower' not in params:\n",
    "        params['lower'] = 0.\n",
    "    if 'upper' not in params:\n",
    "        params['upper'] = None\n",
    "\n",
    "    alpha, lower, upper = params['alpha'], params['lower'], params['upper']\n",
    "\n",
    "    result = np.random.pareto(alpha, size)\n",
    "    result = result.clip(lower, upper)\n",
    "    return result / sum(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "(array([0.49139968, 0.48215841, 0.44653091]),\n",
      " array([0.24703223, 0.24348513, 0.26158784]))\n",
      "[<torch.utils.data.dataset.Subset object at 0x7febad4b5f50>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7febae6c9d50>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7febae5f9190>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7febae5f9c90>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7febae5f9dd0>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7fec6e53bed0>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7fec6e53bf10>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7fec6e53bcd0>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7fec6e53b990>,\n",
      " <torch.utils.data.dataset.Subset object at 0x7fec6e53b9d0>]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    \"\"\"Test `get_norm`\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    trainDataset = dset.CIFAR10(root='cifar', train=True, download=True, transform=transform)\n",
    "    pprint(get_norm(trainDataset))\n",
    "\n",
    "    \"\"\"Test `adv_random_split`\"\"\"\n",
    "    pprint(random_split_by_dist(trainDataset, 10, params={}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}