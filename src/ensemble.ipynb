{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from dist.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import import_ipynb\n",
    "import dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nets: list,\n",
    "        mode: callable = None,\n",
    "        reputations=None\n",
    "    ):\n",
    "        \"\"\"Create an ensemble model and its methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nets : list\n",
    "            List of neural networks (models) .\n",
    "        mode : function\n",
    "            Function. Inputs of the function are outputs from each model.\n",
    "            Outputs of the function are the aggregated results.\n",
    "        reputations : np.array\n",
    "            Sum of `reputations` SHOULD be 1.\n",
    "            `len(reputations)` SHOULD be same as number of `nets`.\n",
    "        \"\"\"\n",
    "\n",
    "        self.nets = nets\n",
    "        self.num_nets = len(self.nets)\n",
    "\n",
    "        mode = mode or avg\n",
    "        if reputations is None:\n",
    "            reputations = dist.uniform(self.num_nets)\n",
    "\n",
    "        self.set_mode(mode)\n",
    "        self.update_reputations(reputations)\n",
    "\n",
    "    def set_mode(self, new_mode: callable):\n",
    "        self.mode = new_mode\n",
    "\n",
    "    def update_reputations(self, new_reputations):\n",
    "        assert len(new_reputations) == self.num_nets, \\\n",
    "            \"dim of `reputations` SHOULD be same as len(nets)\"\n",
    "        assert math.isclose(sum(new_reputations), 1.), \\\n",
    "            \"sum of `reputations` SHOULD be 1.\"\n",
    "        if type(new_reputations) != np.ndarray:  # list, et al.\n",
    "            new_reputations = np.array(new_reputations)  # converts into np.array\n",
    "\n",
    "        self.reputations = new_reputations\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Calculate inference result of this (self) ensemble model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculates inference result\n",
    "        outputs = list()\n",
    "        for net in self.nets:\n",
    "            outputs.append(net(inputs))\n",
    "        outputs = torch.stack(outputs)  # to Tensor\n",
    "\n",
    "        return self.mode(outputs, self.reputations)\n",
    "\n",
    "    def eval(self):\n",
    "        for net in self.nets:\n",
    "            net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(outputs, reputations=None):\n",
    "    if reputations is None:\n",
    "        reputations = dist.uniform(len(outputs))\n",
    "\n",
    "    # Calculates `result` which is the final one\n",
    "    result = torch.empty_like(outputs)\n",
    "    for net_idx, (output, reputation) in enumerate(zip(outputs, reputations)):\n",
    "        result[net_idx] = output.mul(reputation)\n",
    "\n",
    "    return torch.sum(result, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med(outputs, reputations=None):\n",
    "    \"\"\"Calculate weighted median.\n",
    "    \n",
    "    See https://en.wikipedia.org/wiki/Weighted_median for weighted median.\n",
    "    \"\"\"\n",
    "\n",
    "    if reputations is None:\n",
    "        reputations = dist.uniform(len(outputs))\n",
    "\n",
    "    # calculates sorted `outputs`' indexes\n",
    "    selectors = outputs.data.sort(dim=0)[1]  # [0]: values, [1]: indexes\n",
    "    # shape: (num_nets, batch_size, num_classes)\n",
    "    # value: which network (index of net)\n",
    "\n",
    "    # calculates sorted reputations\n",
    "    sorted_repus = torch.from_numpy(reputations)[selectors]\n",
    "\n",
    "    # selects median values\n",
    "    result = torch.empty_like(outputs[0])\n",
    "    # shape: (batch_size, num_classe)\n",
    "\n",
    "    net_max, batch_max, class_max = selectors.shape\n",
    "\n",
    "    for batch_idx in range(batch_max):\n",
    "        for class_idx in range(class_max):\n",
    "\n",
    "            accumulated_repus = 0.\n",
    "\n",
    "            for net_idx in range(net_max):\n",
    "\n",
    "                selector = selectors[net_idx][batch_idx][class_idx]  # index of selected net\n",
    "                accumulated_repus += sorted_repus[net_idx][batch_idx][class_idx]\n",
    "\n",
    "                if accumulated_repus >= 0.5:\n",
    "                    # saves median value at `result` and then `break`\n",
    "                    result[batch_idx][class_idx] = outputs[selector][batch_idx][class_idx]\n",
    "                    break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max(outputs, reputations=None):\n",
    "    \"\"\"Calculate weighted max.\n",
    "    \"\"\"\n",
    "\n",
    "    if reputations is None:\n",
    "        reputations = dist.uniform(len(outputs))\n",
    "\n",
    "    # calculates max indexes via reputations\n",
    "    selectors = torch.empty_like(outputs)\n",
    "    for net_idx, (output, reputation) in enumerate(zip(outputs, reputations)):\n",
    "        selectors[net_idx] = output.mul(reputation)\n",
    "    selectors = selectors.data.max(dim=0)[1]  # [0]: values, [1]: indexes\n",
    "    # shape: (batch_size, num_classes)\n",
    "    # value: which network (index of net)\n",
    "\n",
    "    # selects max values\n",
    "    result = torch.empty_like(outputs[0])\n",
    "    # shape: (batch_size, num_classe)\n",
    "\n",
    "    batch_max, class_max = selectors.shape\n",
    "\n",
    "    for batch_idx in range(batch_max):\n",
    "        for class_idx in range(class_max):\n",
    "            # saves max value at `result`\n",
    "            net_idx = selectors[batch_idx][class_idx]\n",
    "            result[batch_idx][class_idx] = outputs[net_idx][batch_idx][class_idx]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "source": [
    "# main"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from ml.ipynb\n",
      "importing Jupyter notebook from nets.ipynb\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    from torch.utils.data import DataLoader  # TODO: DistributedDataParallel\n",
    "\n",
    "    import import_ipynb\n",
    "    from ml import train, test\n",
    "    import nets\n",
    "\n",
    "    \"\"\"Hyperparams\"\"\"\n",
    "    numNets = 5\n",
    "    numWorkers = 4\n",
    "    cuda = True\n",
    "\n",
    "    base_path = './ensemble_test'\n",
    "\n",
    "    trainFiles = [None for _ in range(numNets)]\n",
    "    testFiles = [None for _ in range(numNets)]\n",
    "    for i in range(numNets):\n",
    "        path = os.path.join(base_path, str(i))\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        trainFiles[i] = open(os.path.join(path, 'train.csv'), 'w')\n",
    "        testFiles[i] = open(os.path.join(path, 'test.csv'), 'w')\n",
    "\n",
    "    epochs = 2\n",
    "    batchSz = 256\n",
    "\n",
    "    \"\"\"Datasets\"\"\"\n",
    "    # # gets mean and std\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    # dataset = dset.CIFAR10(root='cifar', train=True, download=True, transform=transform)\n",
    "    # normMean, normStd = dist.get_norm(dataset)\n",
    "    normMean = [0.49139968, 0.48215841, 0.44653091]\n",
    "    normStd = [0.24703223, 0.24348513, 0.26158784]\n",
    "    normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "    trainTransform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    testTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "\n",
    "    trainset = dset.CIFAR10(root='cifar', train=True, download=True, transform=trainTransform)\n",
    "    testset = dset.CIFAR10(root='cifar', train=False, download=True, transform=trainTransform)\n",
    "\n",
    "    # splits datasets\n",
    "    splited_trainset = dist.random_split_by_dist(\n",
    "        trainset,\n",
    "        size=numNets,\n",
    "        dist=dist.pareto,\n",
    "        alpha=2.\n",
    "    )\n",
    "    splited_testset = dist.random_split_by_dist(\n",
    "        testset,\n",
    "        size=numNets,\n",
    "        dist=dist.pareto,\n",
    "        alpha=2.\n",
    "    )\n",
    "\n",
    "    # num_workers: number of CPU cores to use for data loading\n",
    "    # pin_memory: being able to speed up the host to device transfer by enabling\n",
    "    kwargs = {'num_workers': numWorkers, 'pin_memory': cuda}\n",
    "\n",
    "    # loaders\n",
    "    trainLoaders = [DataLoader(\n",
    "        splited_trainset[i], batch_size=batchSz, shuffle=True, **kwargs\n",
    "    ) for i in range(numNets)]\n",
    "    testLoaders = [DataLoader(\n",
    "        splited_testset[i], batch_size=batchSz, shuffle=True, **kwargs\n",
    "    ) for i in range(numNets)]\n",
    "\n",
    "    \"\"\"Nets\"\"\"\n",
    "    num_classes = 10\n",
    "    resnets = [nets.resnet18(num_classes=num_classes) for _ in range(numNets)]\n",
    "\n",
    "    criterions = [nn.CrossEntropyLoss() for _ in range(numNets)]\n",
    "    optimizers = [optim.SGD(net.parameters(), lr=1e-1, momentum=0.9) for net in resnets]\n",
    "\n",
    "    if cuda:\n",
    "        for net in resnets:\n",
    "            # if multi-gpus\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                net = nn.DataParallel(net)\n",
    "\n",
    "            # use cuda\n",
    "            net.cuda()\n",
    "\n",
    "    \"\"\"Train & Test models\"\"\"\n",
    "    for i in range(numNets):\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            train(\n",
    "                resnets[i], criterions[i], optimizers[i], trainLoaders[i],\n",
    "                epoch=epoch, cuda=cuda, log=True, log_file=trainFiles[i]\n",
    "            )\n",
    "            test(\n",
    "                resnets[i], criterions[i], testLoaders[i],\n",
    "                epoch=epoch, cuda=cuda, log=True, log_file=testFiles[i]\n",
    "            )\n",
    "\n",
    "    \"\"\"Test the ensemble model\"\"\"\n",
    "    ensemble = Ensemble(resnets, mode=med, reputations=[0.05, 0.2, 0.3, 0.4, 0.05])\n",
    "\n",
    "    testFile = open(os.path.join(base_path, 'test.csv'), 'w')\n",
    "\n",
    "    for i in range(numNets):\n",
    "        test(\n",
    "            ensemble, criterions[i], testLoaders[i],\n",
    "            epoch=0, cuda=cuda, log=True, log_file=testFile\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}