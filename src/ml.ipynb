{
 "cells": [
  {
   "source": [
    "# ML functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "## Train\n",
    "\n",
    "### TODO\n",
    "\n",
    "- [ ] logging time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloader,\n",
    "    epoch: int = 0,\n",
    "    cuda: bool = False,\n",
    "    log: bool = False,\n",
    "    log_file = None\n",
    "):\n",
    "    # tells net to do training\n",
    "    net.train()\n",
    "\n",
    "    # for log\n",
    "    nProcessed = 0\n",
    "    nTrain = len(dataloader.dataset)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # sets gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, and opt\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        nProcessed += len(inputs)\n",
    "        pred = outputs.data.max(dim=1)[1]  # get the index of the max log-probability\n",
    "        incorrect = pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "        err = 100. * incorrect / len(inputs)\n",
    "        partialEpoch = epoch + batch_idx / len(dataloader)\n",
    "\n",
    "        if log and (log_file is not None):  # saves at csv file\n",
    "            log_file.write('{},{},{}\\n'.format(partialEpoch, loss.item(), err))\n",
    "            log_file.flush()\n",
    "        else:  # print at STDOUT\n",
    "            print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
    "                partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(dataloader), loss.item(), err\n",
    "            ))"
   ]
  },
  {
   "source": [
    "## Test\n",
    "\n",
    "### TODO\n",
    "\n",
    "- [ ] logging time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    net,\n",
    "    criterion,\n",
    "    dataloader,\n",
    "    epoch: int = 0,\n",
    "    cuda: bool = False,\n",
    "    log: bool = False,\n",
    "    log_file = None\n",
    "):\n",
    "    # tells net to do evaluating\n",
    "    net.eval()\n",
    "\n",
    "    # for log\n",
    "    test_loss = 0.\n",
    "    incorrect = 0.\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # eval\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "            pred = outputs.data.max(dim=1)[1]  # get the index of the max log-probability\n",
    "            incorrect += pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "\n",
    "    # log\n",
    "    test_loss /= len(dataloader)  # loss function already averages over batch size\n",
    "    nTotal = len(dataloader.dataset)\n",
    "    err = 100. * incorrect / nTotal\n",
    "\n",
    "    if log and (log_file is not None):  # saves at csv file\n",
    "        log_file.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
    "        log_file.flush()\n",
    "    else:  # print at STDOUT\n",
    "        print('\\nTest Set\\tAverage Loss: {:.4f}\\tError: {}/{} ({:.06f}%)\\n'.format(\n",
    "            test_loss, incorrect, nTotal, err\n",
    "        ))"
   ]
  },
  {
   "source": [
    "## Save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(\n",
    "    epoch: int,\n",
    "    net,\n",
    "    optimizer,\n",
    "    pth_path: str = 'latest.pth'\n",
    "):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, pth_path)"
   ]
  },
  {
   "source": [
    "## Load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(\n",
    "    pth_path: str = 'latest.pth'\n",
    "):\n",
    "    \"\"\"Use the return value as the following codes:\n",
    "    \n",
    "    checkpoint = load(pth_path=netPath)\n",
    "    epoch = checkpoint['epoch'] + 1  # current epoch\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \"\"\"\n",
    "    return torch.load(pth_path)"
   ]
  },
  {
   "source": [
    "# main"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from nets.ipynb\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    from torch.utils.data import DataLoader  # TODO: DistributedDataParallel\n",
    "\n",
    "    import import_ipynb\n",
    "    import nets\n",
    "\n",
    "    \"\"\"Hyperparams\"\"\"\n",
    "    numWorkers = 4\n",
    "    cuda = True\n",
    "\n",
    "    base_path = './ml_test'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    trainFile = open(os.path.join(base_path, 'train.csv'), 'w')\n",
    "    testFile = open(os.path.join(base_path, 'test.csv'), 'w')\n",
    "    netPath = os.path.join(base_path, 'net.pth')\n",
    "\n",
    "    epochs = 2\n",
    "    batchSz = 256\n",
    "\n",
    "    \"\"\"Datasets\"\"\"\n",
    "    # # gets mean and std\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    # dataset = dset.CIFAR10(root='cifar', train=True, download=True, transform=transform)\n",
    "    # normMean, normStd = utils.getNorm(dataset)\n",
    "    normMean = [0.49139968, 0.48215841, 0.44653091]\n",
    "    normStd = [0.24703223, 0.24348513, 0.26158784]\n",
    "    normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "    trainTransform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    testTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "\n",
    "    # num_workers: number of CPU cores to use for data loading\n",
    "    # pin_memory: being able to speed up the host to device transfer by enabling\n",
    "    kwargs = {'num_workers': numWorkers, 'pin_memory': cuda}\n",
    "\n",
    "    # loaders\n",
    "    trainLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=True, download=True, transform=trainTransform),\n",
    "        batch_size=batchSz, shuffle=True, **kwargs\n",
    "    )\n",
    "    testLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=False, download=True, transform=testTransform),\n",
    "        batch_size=batchSz, shuffle=False, **kwargs\n",
    "    )\n",
    "\n",
    "    \"\"\"Nets\"\"\"\n",
    "    num_classes = 10\n",
    "    net = nets.resnet18(num_classes=num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=1e-1, momentum=0.9)\n",
    "\n",
    "    if cuda:\n",
    "        # if multi-gpus\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # use cuda\n",
    "        net.cuda()\n",
    "\n",
    "    \"\"\"Train & Test & Save\"\"\"\n",
    "    epoch = 0\n",
    "    while True:  # epoch < epochs\n",
    "        # load if exist\n",
    "        if os.path.isfile(netPath):\n",
    "            checkpoint = load(netPath)\n",
    "\n",
    "            epoch = checkpoint['epoch'] + 1  # current epoch\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "        # escape condition\n",
    "        if epoch >= epochs:\n",
    "            break\n",
    "\n",
    "        train(\n",
    "            net, criterion, optimizer, trainLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=trainFile\n",
    "        )\n",
    "        test(\n",
    "            net, criterion, testLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=testFile\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save(epoch, net, optimizer, netPath)\n",
    "\n",
    "        # current epoch\n",
    "        epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}