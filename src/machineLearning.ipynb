{
 "cells": [
  {
   "source": [
    "# ML functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "## Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, dataloader, params: dict = {}):\n",
    "    if 'epoch' not in params:\n",
    "        params['epoch'] = 0\n",
    "    if 'cuda' not in params:\n",
    "        params['cuda'] = False\n",
    "    if 'log' not in params:\n",
    "        params['log'] = False\n",
    "    if 'logFile' not in params:\n",
    "        if params['log']:\n",
    "            params['logFile'] = open('train.csv', 'w')\n",
    "        else:\n",
    "            params['logFile'] = None\n",
    "\n",
    "    epoch, cuda, log, logFile = params['epoch'], params['cuda'], params['log'], params['logFile']\n",
    "\n",
    "    net.train()  # tells net to do training\n",
    "\n",
    "    # for log\n",
    "    nProcessed = 0\n",
    "    nTrain = len(dataloader.dataset)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # sets gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, and opt\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        nProcessed += len(inputs)\n",
    "        pred = outputs.data.max(dim=1)[1]  # get the index of the max log-probability\n",
    "        incorrect = pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "        err = 100. * incorrect / len(inputs)\n",
    "        partialEpoch = params['epoch'] + batch_idx / len(dataloader)\n",
    "\n",
    "        if log and (logFile is not None):  # saves at csv file\n",
    "            logFile.write('{},{},{}\\n'.format(partialEpoch, loss.item(), err))\n",
    "            logFile.flush()\n",
    "\n",
    "        else:  # print at STDOUT\n",
    "            print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
    "                partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(dataloader), loss.item(), err\n",
    "            ), end='\\r')"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, criterion, dataloader, params: dict = {}):\n",
    "    if 'epoch' not in params:\n",
    "        params['epoch'] = 0\n",
    "    if 'cuda' not in params:\n",
    "        params['cuda'] = False\n",
    "    if 'log' not in params:\n",
    "        params['log'] = False\n",
    "    if 'logFile' not in params:\n",
    "        if params['log']:\n",
    "            params['logFile'] = open('test.csv', 'w')\n",
    "        else:\n",
    "            params['logFile'] = None\n",
    "\n",
    "    epoch, cuda, log, logFile = params['epoch'], params['cuda'], params['log'], params['logFile']\n",
    "\n",
    "    net.eval()  # tells net to do evaluating\n",
    "\n",
    "    # for log\n",
    "    test_loss = 0.\n",
    "    incorrect = 0.\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # eval\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "            pred = outputs.data.max(dim=1)[1]  # get the index of the max log-probability\n",
    "            incorrect += pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "\n",
    "    # log\n",
    "    test_loss /= len(dataloader)  # loss function already averages over batch size\n",
    "    nTotal = len(dataloader.dataset)\n",
    "    err = 100. * incorrect / nTotal\n",
    "\n",
    "    if log and (logFile is not None):  # saves at csv file\n",
    "        logFile.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
    "        logFile.flush()\n",
    "    else:  # print at STDOUT\n",
    "        print('\\nTest Set\\tAverage Loss: {:.4f}\\tError: {}/{} ({:.06f}%)\\n'.format(\n",
    "            test_loss, incorrect, nTotal, err\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}