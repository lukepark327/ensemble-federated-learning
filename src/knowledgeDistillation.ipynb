{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n",
    "\n",
    "Computes the Knowledge Distillation (KD) loss and trains based on KD.\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "* alpha\n",
    "* temperature\n",
    "\n",
    "Reference:\n",
    "\n",
    "* https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100\n",
    "* https://github.com/IntelLabs/distiller/blob/master/distiller/knowledge_distillation.py#L135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_KD(outputs, labels, teacher_outputs, params: dict = {}):\n",
    "    if 'alpha' not in params:\n",
    "        params['alpha'] = 0.1\n",
    "    if 'temperature' not in params:\n",
    "        params['temperature'] = 3\n",
    "\n",
    "    alpha, temperature = params['alpha'], params['temperature']\n",
    "\n",
    "    loss_KD = nn.KLDivLoss()(\n",
    "        F.log_softmax(outputs / temperature, dim=1),\n",
    "        F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    ) * (alpha * temperature * temperature) + \\\n",
    "        F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    return loss_KD"
   ]
  },
  {
   "source": [
    "## Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KD(student, teacher, criterion_kd, optimizer, dataloader, params: dict = {}):\n",
    "    \"\"\"Trains `student` network\n",
    "    \n",
    "    Using KD with (pre-trained) `teacher` network\n",
    "    \n",
    "    Refers https://keras.io/examples/vision/knowledge_distillation/ .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student: net\n",
    "        Trained by `teacher`.\n",
    "    teacher: net\n",
    "        Trains `student`.\n",
    "    criterion_kd: function\n",
    "        Loss function. See `criterion_KD`.\n",
    "    optimizer: optimizer\n",
    "        Optimizer.\n",
    "    dataloader: data loader\n",
    "        Data loader.\n",
    "    params: dict\n",
    "        Contains 'epoch', 'cuda', 'log' and 'logFile'.\n",
    "        (Maybe) Contains 'alpha' and 'temperature' which are required\n",
    "        for `criterion_kd` function.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'epoch' not in params:\n",
    "        params['epoch'] = 0\n",
    "    if 'cuda' not in params:\n",
    "        params['cuda'] = False\n",
    "    if 'log' not in params:\n",
    "        params['log'] = False\n",
    "    if 'logFile' not in params:\n",
    "        if params['log']:\n",
    "            params['logFile'] = open('test.csv', 'w')\n",
    "        else:\n",
    "            params['logFile'] = None\n",
    "\n",
    "    epoch, cuda, log, logFile = params['epoch'], params['cuda'], params['log'], params['logFile']\n",
    "\n",
    "    student.train()  # tells student to do training\n",
    "    teacher.eval()  # tells teacher to eval\n",
    "\n",
    "    # for log\n",
    "    nProcessed = 0\n",
    "    nTrain = len(dataloader.dataset)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # sets gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, and opt\n",
    "        outputs, teacher_outputs = student(inputs), teacher(inputs)\n",
    "        loss = criterion_kd(outputs, targets, teacher_outputs, params=params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for log\n",
    "        nProcessed += len(inputs)\n",
    "        pred = outputs.data.max(1)[1]  # get the index of the max log-probability\n",
    "        incorrect = pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "        err = 100. * incorrect / len(inputs)\n",
    "        partialEpoch = epoch + batch_idx / len(dataloader)\n",
    "\n",
    "        if log and (logFile is not None):  # saves at csv file\n",
    "            logFile.write('{},{},{}\\n'.format(partialEpoch, loss.item(), err))\n",
    "            logFile.flush()\n",
    "\n",
    "        else:  # print at STDOUT\n",
    "            print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
    "                partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(dataloader), loss.item(), err\n",
    "            ), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from machineLearning.ipynb\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import torch.optim as optim\n",
    "\n",
    "    import torchvision.models as models\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    from torch.utils.data import DataLoader  # TODO: DistributedDataParallel\n",
    "\n",
    "    import import_ipynb\n",
    "    from machineLearning import test\n",
    "\n",
    "    \"\"\"Hyperparams\"\"\"\n",
    "    numWorkers = 4\n",
    "    cuda = True\n",
    "\n",
    "    base_path = './kd'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    trainFile = open(os.path.join(base_path, 'train.csv'), 'w')\n",
    "    testFile = open(os.path.join(base_path, 'test.csv'), 'w')\n",
    "\n",
    "    epochs = 2\n",
    "    batchSz = 256\n",
    "\n",
    "    \"\"\"Datasets\"\"\"\n",
    "    # # gets mean and std\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    # dataset = dset.CIFAR10(root='cifar', train=True, download=True, transform=transform)\n",
    "    # normMean, normStd = utils.getNorm(dataset)\n",
    "    normMean = [0.49139968, 0.48215841, 0.44653091]\n",
    "    normStd = [0.24703223, 0.24348513, 0.26158784]\n",
    "    normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "    trainTransform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    testTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "\n",
    "    # num_workers: number of CPU cores to use for data loading\n",
    "    # pin_memory: being able to speed up the host to device transfer by enabling\n",
    "    kwargs = {'num_workers': numWorkers, 'pin_memory': cuda}\n",
    "\n",
    "    # loaders\n",
    "    trainLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=True, download=True, transform=trainTransform),\n",
    "        batch_size=batchSz, shuffle=True, **kwargs\n",
    "    )\n",
    "    testLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=False, download=True, transform=testTransform),\n",
    "        batch_size=batchSz, shuffle=False, **kwargs\n",
    "    )\n",
    "\n",
    "    \"\"\"Net\"\"\"\n",
    "    teacher = models.resnet152(pretrained=True)\n",
    "    student = models.resnet18()\n",
    "\n",
    "    if cuda:\n",
    "        teacher = nn.DataParallel(teacher)  # multi-GPUs\n",
    "        teacher.cuda()\n",
    "        student = nn.DataParallel(student)  # multi-GPUs\n",
    "        student.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # student's loss function\n",
    "    optimizer = optim.SGD(student.parameters(), lr=1e-1, momentum=0.9)  # student's\n",
    "\n",
    "    \"\"\"Train & Test\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        train_KD(student, teacher, criterion_KD, optimizer, trainLoader, params={\n",
    "            'epoch': epoch,\n",
    "            'cuda': cuda,\n",
    "            'log': True,\n",
    "            'logFile': trainFile,\n",
    "            'alpha': 0.5,\n",
    "            'temperature': 5\n",
    "        })\n",
    "        test(student, criterion, testLoader, params={\n",
    "            'epoch': 0,\n",
    "            'cuda': cuda,\n",
    "            'log': True,\n",
    "            'logFile': testFile\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}