{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n",
    "\n",
    "Computes the Knowledge Distillation (KD) loss and trains based on KD.\n",
    "\n",
    "Reference:\n",
    "\n",
    "* https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100\n",
    "* https://github.com/IntelLabs/distiller/blob/master/distiller/knowledge_distillation.py#L135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Loss Function for KD\n",
    "\n",
    "Using `batchmean` instead of `mean` at `KLDivLoss()`. See https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_KD(\n",
    "    outputs,\n",
    "    labels,\n",
    "    teacher_outputs,\n",
    "    alpha: float = 0.1,\n",
    "    temperature: float = 3.\n",
    "):\n",
    "    loss_KD = nn.KLDivLoss(reduction='batchmean')(\n",
    "        F.log_softmax(outputs / temperature, dim=1),\n",
    "        F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    ) * (alpha * temperature * temperature) + \\\n",
    "        F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    return loss_KD"
   ]
  },
  {
   "source": [
    "## Train for KD\n",
    "\n",
    "### TODO\n",
    "\n",
    "- [ ] logging time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KD(\n",
    "    student, teacher, criterion_kd, optimizer, dataloader,\n",
    "    epoch: int = 0, cuda: bool = False, log: bool = False, log_file=None,\n",
    "    **params\n",
    "):\n",
    "    \"\"\"Train `student` network.\n",
    "    \n",
    "    Using KD with (pre-trained) `teacher` network.\n",
    "    \n",
    "    Refers https://keras.io/examples/vision/knowledge_distillation/ .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student : net\n",
    "        Trained by `teacher`.\n",
    "    teacher : net\n",
    "        Trains `student`.\n",
    "    criterion_kd : function\n",
    "        Loss function. See `criterion_KD`.\n",
    "    optimizer : optimizer\n",
    "        Optimizer.\n",
    "    dataloader : data loader\n",
    "        Data loader.\n",
    "    epoch : int\n",
    "        Current epoch information for logging.\n",
    "    cuda : bool\n",
    "        Cuda available.\n",
    "    log : bool\n",
    "        Records logs on `log_file` when `log` is True.\n",
    "        Or prints it on STDOUT.\n",
    "    log_file : (file) stream\n",
    "        Files where you want to record logs.\n",
    "    \"\"\"\n",
    "\n",
    "    student.train()  # tells student to do training\n",
    "    teacher.eval()  # tells teacher to eval\n",
    "\n",
    "    # for log\n",
    "    nProcessed = 0\n",
    "    nTrain = len(dataloader.dataset)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # sets gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, and opt\n",
    "        outputs, teacher_outputs = student(inputs), teacher(inputs)\n",
    "        loss = criterion_kd(outputs, targets, teacher_outputs, **params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for log\n",
    "        nProcessed += len(inputs)\n",
    "        pred = outputs.data.max(1)[1]  # get the index of the max log-probability\n",
    "        incorrect = pred.ne(targets.data).cpu().sum()  # ne: not equal\n",
    "        err = 100. * incorrect / len(inputs)\n",
    "        partialEpoch = epoch + batch_idx / len(dataloader)\n",
    "\n",
    "        if log and (log_file is not None):  # saves at csv file\n",
    "            log_file.write('{},{},{}\\n'.format(partialEpoch, loss.item(), err))\n",
    "            log_file.flush()\n",
    "        else:  # print at STDOUT\n",
    "            print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n",
    "                partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(dataloader), loss.item(), err\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from ml.ipynb\n",
      "importing Jupyter notebook from nets.ipynb\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    import torch\n",
    "    import torch.optim as optim\n",
    "\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    from torch.utils.data import DataLoader  # TODO: DistributedDataParallel\n",
    "\n",
    "    import import_ipynb\n",
    "    from ml import train, test, save\n",
    "    import nets\n",
    "\n",
    "    \"\"\"Hyperparams\"\"\"\n",
    "    numWorkers = 4\n",
    "    cuda = True\n",
    "\n",
    "    base_path = './kd_test'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    teacher_path = os.path.join(base_path, 'teacher')\n",
    "    os.makedirs(teacher_path, exist_ok=True)\n",
    "    teacher_trainFile = open(os.path.join(teacher_path, 'train.csv'), 'w')\n",
    "    teacher_testFile = open(os.path.join(teacher_path, 'test.csv'), 'w')\n",
    "    teacher_netPath = os.path.join(teacher_path, 'net.pth')\n",
    "\n",
    "    student_path = os.path.join(base_path, 'student')\n",
    "    os.makedirs(student_path, exist_ok=True)\n",
    "    trainFile = open(os.path.join(student_path, 'train.csv'), 'w')\n",
    "    testFile = open(os.path.join(student_path, 'test.csv'), 'w')\n",
    "\n",
    "    epochs = 2\n",
    "    teacher_epochs = 10\n",
    "    batchSz = 256\n",
    "\n",
    "    \"\"\"Datasets\"\"\"\n",
    "    # # gets mean and std\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    # dataset = dset.CIFAR10(root='cifar', train=True, download=True, transform=transform)\n",
    "    # normMean, normStd = dist.get_norm(dataset)\n",
    "    normMean = [0.49139968, 0.48215841, 0.44653091]\n",
    "    normStd = [0.24703223, 0.24348513, 0.26158784]\n",
    "    normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "    trainTransform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    testTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "\n",
    "    # num_workers: number of CPU cores to use for data loading\n",
    "    # pin_memory: being able to speed up the host to device transfer by enabling\n",
    "    kwargs = {'num_workers': numWorkers, 'pin_memory': cuda}\n",
    "\n",
    "    # loaders\n",
    "    trainLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=True, download=True, transform=trainTransform),\n",
    "        batch_size=batchSz, shuffle=True, **kwargs\n",
    "    )\n",
    "    testLoader = DataLoader(\n",
    "        dset.CIFAR10(root='cifar', train=False, download=True, transform=testTransform),\n",
    "        batch_size=batchSz, shuffle=False, **kwargs\n",
    "    )\n",
    "\n",
    "    \"\"\"Nets\"\"\"\n",
    "    num_classes = 10\n",
    "\n",
    "    \"\"\"Define `teacher`\"\"\"\n",
    "    # \"\"\"Transfer Learning\"\"\"\n",
    "    # teacher = nets.resnext101_32x8d(pretrained=True)\n",
    "\n",
    "    # # fixes parameters for `teacher`\n",
    "    # for param in teacher.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # # replaces nodes (num_classes) of output layer (fc)\n",
    "    # num_ftrs = teacher.fc.in_features\n",
    "    # teacher.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # # params to learn\n",
    "    # params_to_update = []\n",
    "    # for param in teacher.parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         params_to_update.append(param)\n",
    "\n",
    "    teacher = nets.resnet18(num_classes=num_classes)\n",
    "\n",
    "    teacher_criterion = nn.CrossEntropyLoss()\n",
    "    teacher_optimizer = optim.SGD(teacher.parameters(), lr=1e-1, momentum=0.9)\n",
    "\n",
    "    \"\"\"Define `student`\"\"\"\n",
    "    student = nets.resnet18(num_classes=num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(student.parameters(), lr=1e-1, momentum=0.9)\n",
    "\n",
    "    if cuda:\n",
    "        # if multi-gpus\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            teacher = nn.DataParallel(teacher)\n",
    "            student = nn.DataParallel(student)\n",
    "\n",
    "        # use cuda\n",
    "        teacher.cuda()\n",
    "        student.cuda()\n",
    "\n",
    "    \"\"\"Train & Test `teacher`\"\"\"\n",
    "    for epoch in range(teacher_epochs):\n",
    "        train(\n",
    "            teacher, teacher_criterion, teacher_optimizer, trainLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=teacher_trainFile\n",
    "        )\n",
    "        test(\n",
    "            teacher, criterion, testLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=teacher_testFile\n",
    "        )\n",
    "        save(epoch, teacher, teacher_optimizer, teacher_netPath)\n",
    "\n",
    "    \"\"\"Train & Test `student`\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        train_KD(\n",
    "            student, teacher, criterion_KD, optimizer, trainLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=trainFile,\n",
    "            alpha=0.9, temperature=4\n",
    "        )\n",
    "        test(\n",
    "            student, criterion, testLoader,\n",
    "            epoch=epoch, cuda=cuda, log=True, log_file=testFile\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}